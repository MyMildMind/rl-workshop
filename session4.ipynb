{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 4: Policy-gradient Pong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #uncomment only if you're running from google colab\n",
    "# !git clone https://github.com/Datatouille/rl-workshop\n",
    "# !mv rl-workshop/* .\n",
    "# !ls\n",
    "# !pip install gym[atari] #For full installations, see https://github.com/openai/gym#installation\n",
    "# !pip install numpy torchvision_nightly\n",
    "# !pip install torch_nightly -f https://download.pytorch.org/whl/nightly/cu92/torch_nightly.html\n",
    "# import torch\n",
    "# torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#environments\n",
    "import gym\n",
    "from solutions.environments import Gridworld\n",
    "\n",
    "#torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "#use cuda 0 if available; assuming 1 gpu\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#misc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "MODEL_PATH = 'models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments we have solved so far:\n",
    "* Gridworld\n",
    "* Blackjack-v0\n",
    "* Taxi-v2\n",
    "\n",
    "**Coding Assignment** What do they have in common? Find out what are the states and actions of each environment above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWrite your code here\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create environments\n",
    "gridworld_env = Gridworld()\n",
    "bj_env = gym.make('Blackjack-v0')\n",
    "taxi_env = gym.make('Taxi-v2')\n",
    "\n",
    "'''\n",
    "Write your code here\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hints** \n",
    "* `env.reset()` returns the initial state of an environment.\n",
    "* `env.action_space` returns the action space of the environment.\n",
    "* `env.observation_space` or `env.state_space` returns the state space of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Got You Here Will Not Take You There"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far our methods of finding the optimal policy has been:\n",
    "\n",
    "1. Predict the action values given a state using either **a sample of transitions (Monte Carlo)** or **previous action values (Temporal Difference)**.\n",
    "2. Store those values in a dictionary `Q[state][action]`\n",
    "3. To perform an optimal action, choose the action index which gives the highest action value in the dictionary.\n",
    "\n",
    "But what if we have infinitely many states?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Pong-v0](https://github.com/openai/gym/blob/master/gym/envs/atari/atari_env.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most influential articles on deep reinforcement is Andrej Karpathy's [Deep Reinforcement Learning: Pong from Pixels](https://karpathy.github.io/2016/05/31/rl/), where he details how to train an agent that learns to play Atari Pong from raw pixels using a technique called **policy gradient**. This session will borrow heavily from this article as well as [a lecture by David Silver](http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching_files/pg.pdf).\n",
    "\n",
    "If you are too young to be familiar with Pong, try it [here](http://www.pongonline.net/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/YOW8m2YGtRg\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/YOW8m2YGtRg\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding Assignment** Before we dive in, let us do some warm-up exercises of finding out more about `PongDeterministic-v0`\n",
    "\n",
    "* State space\n",
    "* Action space\n",
    "* Rewards\n",
    "* How many episodes on average with random actions\n",
    "* Render one frame and see how it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each action index means: ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nWrite your code here\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create environment\n",
    "env = gym.make('PongDeterministic-v0')\n",
    "env.reset()\n",
    "print(f'Each action index means: {env.unwrapped.get_action_meanings()}')\n",
    "\n",
    "'''\n",
    "Write your code here\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Policy Gradient aka Vanilla Policy Gradient REINFORCE aka Stochastic Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gist of it is that instead of using the three steps we have done so far, we do:\n",
    "\n",
    "1. Get rid of the Q dictionary altogether.\n",
    "2. Use a neural network called **policy network** that takes the input of state (pixels) and outputs which action to take as our policy.\n",
    "3. Get **a sample of transitions and discounted rewards** based on that neural network.\n",
    "4. Optimize the policy network to maximize the **expected discounted rewards** using **gradient ascent**.\n",
    "\n",
    "<img src=\"img/pong_pg.png\" alt=\"Pong Policy Gradient\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of taking the entire height (210) * width (160) * color channels (3) = 100,800 pixels for input, we prepocess to a black-and-white 80 * 80 image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2199cdce48>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "for i in range(50): \n",
    "    action = np.random.choice([2,3])\n",
    "    state, reward, done, info = env.step(action)\n",
    "pre_img = state\n",
    "print(pre_img.shape)\n",
    "plt.imshow(pre_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapted from https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5\n",
    "def preprocess_state(I):\n",
    "    I = I[35:195] # crop\n",
    "    I = I[::2,::2,0] # downsample by factor of 2\n",
    "    I[I == 144] = 0 # erase background (background type 1)\n",
    "    I[I == 109] = 0 # erase background (background type 2)\n",
    "    I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    res = I.astype(np.float).reshape(-1)\n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6400,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2199cc3940>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "post_img = preprocess_state(pre_img)\n",
    "print(post_img.shape)\n",
    "plt.imshow(post_img.reshape(80,80), 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a policy network that takes an input of state and output the probability of taking an action directly. That is, we are not estimating the action value function $Q(s,a)$ but directly the policy function $\\pi(a|s)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size,hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size,output_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x),1)\n",
    "        return(x)\n",
    "    \n",
    "#initiate random policy\n",
    "policy = PolicyNet(6400,512,2)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6400])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = env.reset()\n",
    "x = torch.FloatTensor(preprocess_state((state)))[None]\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4939, 0.5061]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#policy probability; pi\n",
    "#function output log softmax\n",
    "pi = policy(x)\n",
    "pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#choose action; +2 because the indice we want are 2 and 3\n",
    "np.random.choice([2,3], p = pi.squeeze().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding Assignment** Use the randomly initiated policy network to play the game for 30 episodes. \n",
    "* What is the average reward across all episodes?\n",
    "* How many times the agent chose to go left or right?\n",
    "\n",
    "Note that in line with [Karpathy's implementation](https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5), we will be using the difference of frames as our state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "actions = []\n",
    "\n",
    "for i in range(30):\n",
    "    #initial state\n",
    "    state = preprocess_state(env.reset())\n",
    "    previous_state = None\n",
    "    \n",
    "    #start game\n",
    "    env.step(1)\n",
    "    #do some random steps\n",
    "    for i in range(10): \n",
    "        action = np.random.choice([2,3])\n",
    "        env.step(action)\n",
    "    \n",
    "    score = 0\n",
    "    while True:\n",
    "        #get input state as difference between two states\n",
    "        current_state = state\n",
    "        input_state = current_state - previous_state if previous_state is not None else np.zeros(80*80)\n",
    "        previous_state = current_state\n",
    "        #convert input to tensor\n",
    "        state_tensor = torch.FloatTensor(input_state)[None]\n",
    "\n",
    "        '''\n",
    "        Write codes to get probability and choose action here\n",
    "        '''\n",
    "        \n",
    "        #record action\n",
    "        actions.append(action)\n",
    "        #environment step\n",
    "        state, reward, done, info = env.step(action)\n",
    "        #record score\n",
    "        score+=reward\n",
    "        #preprocess new state\n",
    "        state = preprocess_state(state)\n",
    "        if done: break\n",
    "    #append scores\n",
    "    scores.append(score)\n",
    "    \n",
    "plt.hist(scores)\n",
    "print(np.mean(scores))\n",
    "print(Counter(actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Monte Carlo methods, we collect all state-action pairs in an episode then update the Q dictionary once the episode ends. It is not very efficient since we always have to wait for the end of the episode. Here instead we will come up with an arbitrary number of timesteps and treat it as though it is an episode, calling it **trajectory**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for a single episode\n",
    "def get_trajectories(policy, env, t_max=300):\n",
    "    trajectories = []\n",
    "\n",
    "    #initialize state\n",
    "    state = preprocess_state(env.reset())\n",
    "    previous_state = None\n",
    "    \n",
    "    #start game\n",
    "    env.step(1)\n",
    "    #do some random steps\n",
    "    for i in range(10): \n",
    "        action = np.random.choice([2,3])\n",
    "        env.step(action)\n",
    "\n",
    "    while True:\n",
    "        #each trajectory\n",
    "        states= []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        probs = []\n",
    "        for t in range(t_max):\n",
    "            #get input state as difference between two states\n",
    "            current_state = state\n",
    "            input_state = current_state - previous_state if previous_state is not None else np.zeros(80*80)\n",
    "            previous_state = current_state\n",
    "            #convert input to tensor\n",
    "            state_tensor = torch.FloatTensor(input_state)[None]\n",
    "            #get probability of doing an action\n",
    "            prob = policy(state_tensor).squeeze().detach().numpy()\n",
    "            #select action based on probability\n",
    "            action = np.random.choice([2,3],p=prob)\n",
    "            #environment step\n",
    "            state, reward, done, info = env.step(action)\n",
    "\n",
    "            #append trajectories\n",
    "            probs.append(prob[action-2])\n",
    "            states.append(input_state)\n",
    "            actions.append(action-2)\n",
    "            rewards.append(reward)\n",
    "            #preprocess next state\n",
    "            state = preprocess_state(state)\n",
    "            if done: break\n",
    "        if done: break\n",
    "        trajectories.append((probs, states,actions,rewards))\n",
    "    return(trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectories = get_trajectories(policy,env)\n",
    "len(trajectories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Ascent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last and most important step is to optimize the policy network so that given a state, it will be more likely to perform an action that would maximize the **expected discounted rewards** $U(\\theta)$:\n",
    "\n",
    "$$U(\\theta) = \\sum_{\\tau} P(\\tau;\\theta) R(\\tau) = \\frac{1}{m}\\sum_{i=1}^{m} \\sum_{t=0}^{H}log\\pi_{\\theta}(a^i_t|s^i_t)R(\\tau^i) = \\frac{1}{m}\\sum_{i=1}^{m} \\sum_{t=0}^{H}log\\pi_{\\theta}(a^i_t|s^i_t)R(\\tau^i)$$\n",
    "\n",
    "where\n",
    "* $\\tau$ is a trajectory\n",
    "* $\\theta$ is a set of parameters for the policy network\n",
    "* $P(\\tau; \\theta)$ is the probability of trajectory $\\tau$ given parameters $\\theta$\n",
    "* $R(\\tau)$ is the discounted rewards of trajectory $\\tau$\n",
    "* $i$ denotes a trajectory\n",
    "* $t$ denotes a timestep within a trajectory\n",
    "\n",
    "In human terms, \n",
    "1. Collect some trajectories with some number of timesteps, say 300 since it's about half the game.\n",
    "2. For each trajectory we have, find the sum of log(probability of action we did) * discounted rewards of that trajectory.\n",
    "3. Decide if how often you want to update your policy network. For instance, if you want to update every 5 trajectories, average 2. calculated from all 5 of them.\n",
    "4. Perform gradient ascent to update the policy network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surrogate(policy, trajectory, discount = 0.995, beta=0.01):\n",
    "    #expand trajectories\n",
    "    probs,states,actions,rewards = trajectory\n",
    "\n",
    "    #calculate normalized future rewards\n",
    "    #get discount rates\n",
    "    discount = discount**np.arange(len(rewards))\n",
    "    #get discounted rewards and expand to number of timesteps collected\n",
    "    rewards = np.array(rewards)*discount[:,np.newaxis]\n",
    "    #get discounted rewards for each timestep\n",
    "    rewards_future = rewards[::-1].cumsum(axis=0)[::-1]\n",
    "    #normalize the rewards to stabilize things\n",
    "    mean = np.mean(rewards_future, axis=1)\n",
    "    std = np.std(rewards_future, axis=1) + 1.0e-10\n",
    "    rewards_normalized = (rewards_future - mean[:,np.newaxis])/std[:,np.newaxis]\n",
    "\n",
    "    #convert trajectory into tensors\n",
    "    actions = torch.tensor(actions, dtype=torch.long, device=device)\n",
    "    old_probs = torch.tensor(probs, dtype=torch.float, device=device)\n",
    "    rewards = torch.tensor(rewards_normalized, dtype=torch.float, device=device)\n",
    "    states = torch.tensor(states, dtype=torch.float, device=device)\n",
    "    new_probs = policy(states).gather(1,actions.reshape(-1,1)).squeeze()\n",
    "\n",
    "    #ratio or log\n",
    "    ratio = new_probs/old_probs\n",
    "#     ratio = torch.log(new_probs)\n",
    "\n",
    "    #regularization term\n",
    "    reg = -(new_probs*torch.log(old_probs+1.e-10)+ (1.0-new_probs)*torch.log(1.0-old_probs+1.e-10))\n",
    "    return(torch.mean(ratio*rewards + beta*reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0069, grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectory = trajectories[0]\n",
    "L = surrogate(policy,trajectory)\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 100/3000 [03:01<1:27:34,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-14.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 200/3000 [05:50<1:21:46,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-18.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 300/3000 [08:30<1:16:32,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-18.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 301/3000 [08:31<1:16:30,  1.70s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-36d81420f21d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# collect trajectories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrajectories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_trajectories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m700\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-4cd43fb40fda>\u001b[0m in \u001b[0;36mget_trajectories\u001b[0;34m(policy, env, t_max)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m#environment step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;31m#append trajectories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/atari_py/ale_python_interface.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0male_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgame_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "episode = 3000\n",
    "epsilon = 0.1\n",
    "beta = .01\n",
    "# keep track of progress\n",
    "scores = []\n",
    "\n",
    "for e in trange(episode):\n",
    "    # collect trajectories\n",
    "    trajectories = get_trajectories(policy,env,t_max=700)\n",
    "    score = 0\n",
    "    \n",
    "    #update with each trajectory\n",
    "    for trajectory in trajectories:\n",
    "        probs, states, actions, rewards = trajectory\n",
    "        score+= np.sum(rewards)\n",
    "        L = -surrogate_clipped(policy, trajectory, epsilon=epsilon, beta=beta)\n",
    "\n",
    "        #run optimizer\n",
    "        optimizer.zero_grad()\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    #record score\n",
    "    scores.append(score)\n",
    "    if (e+1) % 100 ==0: \n",
    "        last_hundred = np.mean(scores[-100:])\n",
    "        print(last_hundred)\n",
    "        if last_hundred > 0: print(f'Solved at {e}')\n",
    "    \n",
    "    #decrease exploration as time goes on\n",
    "    beta*=.995\n",
    "    epsilon*=.999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: How to Derive Gradients of Policy Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/reinforce_derivation1.png\" alt=\"Derivation 1\"/>\n",
    "<img src=\"img/reinforce_derivation2.png\" alt=\"Derivation 2\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proximal Policy Optimization (PPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surrogate_clipped(policy, trajectory, discount = 0.995, epsilon=0.1, beta=0.01):\n",
    "    #expand trajectories\n",
    "    probs, states,actions,rewards = trajectory\n",
    "\n",
    "    #calculate normalized future rewards\n",
    "    #get discount rates\n",
    "    discount = discount**np.arange(len(rewards))\n",
    "    #get discounted rewards and expand to number of timesteps collected\n",
    "    rewards = np.array(rewards)*discount[:,np.newaxis]\n",
    "    #get discounted rewards for each timestep\n",
    "    rewards_future = rewards[::-1].cumsum(axis=0)[::-1]\n",
    "    #normalize the rewards to stabilize things\n",
    "    mean = np.mean(rewards_future, axis=1)\n",
    "    std = np.std(rewards_future, axis=1) + 1.0e-10\n",
    "    rewards_normalized = (rewards_future - mean[:,np.newaxis])/std[:,np.newaxis]\n",
    "\n",
    "    #convert trajectory into tensors\n",
    "    actions = torch.tensor(actions, dtype=torch.long, device=device)\n",
    "    old_probs = torch.tensor(probs, dtype=torch.float, device=device)\n",
    "    rewards = torch.tensor(rewards_normalized, dtype=torch.float, device=device)\n",
    "    states = torch.tensor(states, dtype=torch.float, device=device)\n",
    "    new_probs = policy(states).gather(1,actions.reshape(-1,1)).squeeze()\n",
    "\n",
    "    #ratio or log\n",
    "    ratio = new_probs/old_probs\n",
    "#     ratio = torch.log(new_probs)\n",
    "\n",
    "    #clip ratio; the only difference between REINFORCE and PPO\n",
    "    ratio_clipped = torch.clamp(ratio, 1-epsilon, 1+epsilon)\n",
    "    ratio_final = torch.min(ratio*rewards, ratio_clipped*rewards)\n",
    "    \n",
    "    #regularization term\n",
    "    reg = -(new_probs*torch.log(old_probs+1.e-10)+ (1.0-new_probs)*torch.log(1.0-old_probs+1.e-10))\n",
    "    return(torch.mean(ratio_final*rewards + beta*reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0069, grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectory = trajectories[0]\n",
    "L = surrogate_clipped(policy,trajectory)\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pong Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(210, 160, 3), Discrete(6))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:13<00:00,  2.24it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "764.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "action = np.random.choice([2,3])\n",
    "ep_lens = []\n",
    "\n",
    "for i in trange(30):\n",
    "    env.reset()\n",
    "    ep_len = 0\n",
    "    while True:\n",
    "        ep_len +=1\n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done: break\n",
    "    ep_lens.append(ep_len)\n",
    "    \n",
    "plt.hist(ep_lens)\n",
    "np.mean(ep_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2539a2bfd0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
